{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dbce5d-f3e3-4c82-92ee-0f64b83e51bb",
   "metadata": {},
   "source": [
    "# Infer-Retrieve-Rerank Llama Pack\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama-hub/blob/main/llama_hub/llama_packs/research/infer_retrieve_rerank/infer_retrieve_rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This is our implementation of the paper [\"In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/pdf/2401.12178.pdf) by Oosterlinck et al.\n",
    "\n",
    "The paper proposes \"infer-retrieve-rerank\", a simple paradigm using frozen LLM/retriever models that can do \"extreme\"-label classification (the label space is huge).\n",
    "1. Given a user query, use an LLM to predict an initial set of labels.\n",
    "2. For each prediction, retrieve the actual label from the corpus.\n",
    "3. Given the final set of labels, rerank them using an LLM.\n",
    "\n",
    "All of these can be implemented as LlamaIndex abstractions. In this notebook we show you how to build \"infer-retrieve-rerank\" from scratch but also how to build it as a LlamaPack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d245ce-1f4a-45f8-a608-4941c8cb94b8",
   "metadata": {},
   "source": [
    "## Try out a Dataset\n",
    "\n",
    "We use the BioDEX dataset as mentioned in the paper.\n",
    "\n",
    "Here is the [link to the paper](https://arxiv.org/pdf/2305.13395.pdf). Here is the [link to the Github repo](https://github.com/KarelDO/BioDEX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62fa4b-7322-4381-9703-c3c090b6eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"BioDEX/raw_dataset\")\n",
    "import datasets\n",
    "\n",
    "# load the report-extraction dataset\n",
    "dataset = datasets.load_dataset(\"BioDEX/BioDEX-ICSR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f0ea579-f534-42de-b7d9-79f7260dda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 9624\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 2407\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 3628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cecfa23d-f263-44e3-bd07-7aa0ccbc2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_tokenizer\n",
    "import re\n",
    "from typing import Set\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "\n",
    "sample_size = 5\n",
    "\n",
    "def get_reactions_row(raw_target: str) -> List[str]:\n",
    "    \"\"\"Get reactions from a single row.\"\"\"\n",
    "    reaction_pattern = re.compile(r\"reactions:\\s*(.*)\")\n",
    "    reaction_match = reaction_pattern.search(raw_target)\n",
    "    if reaction_match:\n",
    "        reactions = reaction_match.group(1).split(\",\")\n",
    "        reactions = [r.strip().lower() for r in reactions]\n",
    "    else:\n",
    "        reactions = []\n",
    "    return reactions\n",
    "\n",
    "\n",
    "def get_reactions_set(dataset) -> Set[str]:\n",
    "    \"\"\"Get set of all reactions.\"\"\"\n",
    "    reactions = set()\n",
    "    for data in dataset[\"train\"]:\n",
    "        reactions.update(set(get_reactions_row(data[\"target\"])))\n",
    "    return reactions\n",
    "\n",
    "\n",
    "def get_samples(dataset, sample_size: int = 5):\n",
    "    \"\"\"Get processed sample.\n",
    "\n",
    "    Contains source text and also the reaction label.\n",
    "\n",
    "    Parse reaction text to specifically extract reactions.\n",
    "    \n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for idx, data in enumerate(dataset[\"train\"]):\n",
    "        if idx >= sample_size:\n",
    "            break\n",
    "        text = data[\"fulltext_processed\"]\n",
    "        raw_target = data[\"target\"]\n",
    "\n",
    "        reactions = get_reactions_row(raw_target)\n",
    "\n",
    "        samples.append({\n",
    "            \"text\": text,\n",
    "            \"reactions\": reactions\n",
    "        })\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca770f-8a51-4206-a11c-dc59b1b90a1c",
   "metadata": {},
   "source": [
    "### Index each Reaction with a Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6f7ffea-d698-4fee-8609-92326a0bb7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/c3h91d9s49xblwfvz79s78_c0000gn/T/ipykernel_29743/1748569963.py:4: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random.sample(all_reactions, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fixed eruption',\n",
       " 'abdominal compartment syndrome',\n",
       " 'foreign body reaction',\n",
       " 'intermittent claudication',\n",
       " 'chorioretinal disorder']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "all_reactions = get_reactions_set(dataset)\n",
    "random.sample(all_reactions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c15f5ce-eb3c-4776-b7ad-47f0e08f06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "reaction_nodes = [TextNode(text=r) for r in all_reactions]\n",
    "pipeline = IngestionPipeline(transformations=[OpenAIEmbedding()])\n",
    "reaction_nodes = await pipeline.arun(documents=reaction_nodes)\n",
    "\n",
    "index = VectorStoreIndex(reaction_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3314e-06e7-46aa-aa70-cd726238cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_nodes[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a9a312d-8197-429e-a766-55edcbcb56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62d5f304-4e42-491d-a81e-c3a8c47e008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='e6412990-2c03-40d1-bdb9-586e349b1293', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='abdominal pain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9158768529615556),\n",
       " NodeWithScore(node=TextNode(id_='cf067b02-9418-4588-8f98-6d91f91499b4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='abdominal symptom', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9104600840524487)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reaction_retriever.retrieve('abdominal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6a677-8306-4b8f-b009-3a1992e9ad22",
   "metadata": {},
   "source": [
    "## Define Infer-Retrieve-Rerank Pipeline\n",
    "\n",
    "Here we define the core components needed for the infer-retrieve-rerank pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2790e307-7e48-4f6b-ac30-ca261c98a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.llms.llm import LLM\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.query_pipeline import QueryPipeline\n",
    "from llama_index.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.output_parsers import ChainableOutputParser\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b94016bc-8af6-4714-b713-1662ec127c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_prompt_str = \"\"\"\\\n",
    "\n",
    "Your job is to output a list of predictions given context from a given piece of text. The text context,\n",
    "and information regarding the set of valid predictions is given below. \n",
    "\n",
    "Return the predictions as a comma-separated list of strings.\n",
    "\n",
    "Text Context:\n",
    "{doc_context}\n",
    "\n",
    "Prediction Info:\n",
    "{pred_context}\n",
    "\n",
    "Predictions: \"\"\"\n",
    "\n",
    "infer_prompt = PromptTemplate(infer_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3589b9e6-cfb5-4c60-800d-923ea042bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredsOutputParser(ChainableOutputParser):\n",
    "    \"\"\"Predictions output parser.\"\"\"\n",
    "\n",
    "    def parse(self, output: str) -> List[str]:\n",
    "        \"\"\"Parse predictions.\"\"\"\n",
    "        tokens = output.split(\",\")\n",
    "        return [t.strip() for t in tokens]\n",
    "\n",
    "preds_output_parser = PredsOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b29811f-356e-433d-a159-ebc711d5f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_str = \"\"\"\\\n",
    "Given a piece of text, rank the {num} passages above based on their relevance \\\n",
    "to this piece of text. The passages \\\n",
    "should be listed in descending order using identifiers. \\\n",
    "The most relevant passages should be listed first. \\\n",
    "The output format should be [] > [], e.g., [1] > [2]. \\\n",
    "Only response the ranking results, \\\n",
    "do not say any word or explain. \\\n",
    "\n",
    "Here is a given piece of text: {query}. \n",
    "\n",
    "\"\"\"\n",
    "rerank_prompt = PromptTemplate(rerank_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4c9a4ce-28d8-4bc1-a4f6-335328c7c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_retrieve_rerank(\n",
    "    query: str,\n",
    "    retriever: BaseRetriever,\n",
    "    llm: LLM,\n",
    "    pred_context: str,\n",
    "    reranker_top_n: int = 3\n",
    "):\n",
    "    \"\"\"Infer retrieve rerank.\"\"\"\n",
    "    infer_prompt_c = infer_prompt.as_query_component(\n",
    "        partial={\"pred_context\": pred_context}\n",
    "    )\n",
    "    infer_pipeline = QueryPipeline(chain=[infer_prompt_c, llm, preds_output_parser])\n",
    "    preds = infer_pipeline.run(query)\n",
    "\n",
    "    print(f\"PREDS: {preds}\")\n",
    "    all_nodes = []\n",
    "    for pred in preds:\n",
    "        nodes = retriever.retrieve(str(pred))\n",
    "        all_nodes.extend(nodes)\n",
    "\n",
    "    reranker = RankGPTRerank(\n",
    "        llm=llm,\n",
    "        top_n=reranker_top_n,\n",
    "        rankgpt_rerank_prompt=rerank_prompt,\n",
    "        # verbose=True,\n",
    "    )\n",
    "    reranked_nodes = reranker.postprocess_nodes(all_nodes, query_str=query)\n",
    "    return [n.get_content() for n in reranked_nodes]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea0639-b929-4754-a836-6199e36b970d",
   "metadata": {},
   "source": [
    "### Run Over Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "35269968-d98a-4e77-a320-536f146e58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(dataset, sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e7498f3-6adc-4200-8b12-76cd7939cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "PREDS: ['fluid overload', 'acute respiratory distress syndrome', 'anxiety', 'delirium', 'myocardial insufficiency', 'hypervolemia', 'hypovolemia', 'pneumonia', 'pleural effusion', 'heart failure', 'respiratory distress', 'allergic reaction', 'eosinophilia', 'diarrhea', 'rash']\n",
      "After Reranking, new rank list for nodes: [0, 2, 20, 18, 9, 10, 8, 16, 14, 26, 27, 24, 25, 29, 28, 6, 7, 4, 5, 12, 11, 13, 1, 3, 21, 22, 23, 15, 17, 19]"
     ]
    }
   ],
   "source": [
    "reaction_retriever = index.as_retriever(similarity_top_k=2)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "pred_context = \"\"\"\\\n",
    "The output predictins should be a list of comma-separated adverse \\\n",
    "drug reactions. \\\n",
    "\"\"\"\n",
    "\n",
    "reranker_top_n = 10\n",
    "\n",
    "pred_reactions = []\n",
    "gt_reactions = []\n",
    "for idx, sample in enumerate(samples):\n",
    "    print(idx)\n",
    "    cur_pred_reactions = infer_retrieve_rerank(\n",
    "        sample[\"text\"],\n",
    "        reaction_retriever,\n",
    "        llm,\n",
    "        pred_context,\n",
    "        reranker_top_n=reranker_top_n\n",
    "    )\n",
    "    cur_gt_reactions = sample[\"reactions\"]\n",
    "\n",
    "    pred_reactions.append(cur_pred_reactions)\n",
    "    gt_reactions.append(cur_gt_reactions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4717069f-19d0-43aa-afeb-e0cb17cb1d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fluid overload',\n",
       " 'acute respiratory distress syndrome',\n",
       " 'respiratory distress',\n",
       " 'cardiac failure',\n",
       " 'myocardial infarction',\n",
       " 'hypervolaemia',\n",
       " 'cardiovascular insufficiency',\n",
       " 'pleural effusion',\n",
       " 'pneumonia',\n",
       " 'diarrhoea']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_reactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5223f8e2-c08b-459d-9ded-b25b90899616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diarrhoea', 'drug hypersensitivity', 'rash']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_reactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c81ec8-7b00-4cf7-899f-d0d812a57c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hub",
   "language": "python",
   "name": "llama_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
